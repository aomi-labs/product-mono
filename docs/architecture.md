# AOMI Architecture

AOMI is an AI-powered blockchain operations assistant that transforms natural language into Web3 operations. Built with a streaming-first, modular design, it enables conversational interactions for querying blockchain state, executing swaps, calling smart contracts, and sending transactions.

## System Overview

```mermaid
graph TB
    subgraph "User Interfaces"
        CLI[CLI]
        TUI[TUI]
        WEB[Web Frontend]
    end

    subgraph "API Layer"
        HTTP[HTTP Backend<br/>Axum Server]
        SSE[SSE Stream<br/>/api/updates]
    end

    subgraph "Core Runtime"
        SM[Session Manager]
        CHAT[ChatApp<br/>LLM Orchestration]
        SCHED[Tool Scheduler<br/>IO Bus]
        SEQ[SystemEventQueue]
    end

    subgraph "Agentic Applications"
        DEFAULT[ChatApp<br/>General Assistant]
        FORGE[ForgeApp<br/>Contract Dev]
        L2B[L2BeatApp<br/>Protocol Analysis]
    end

    subgraph "External Services"
        ANTHROPIC[Anthropic Claude]
        MCP[MCP Server]
        ETHERSCAN[Etherscan API]
        RPC[RPC Nodes]
        BAML[BAML Server]
    end

    CLI --> SM
    TUI --> SM
    WEB --> HTTP
    HTTP --> SM
    SM --> SSE

    SM --> DEFAULT
    SM --> FORGE
    SM --> L2B

    DEFAULT --> CHAT
    FORGE --> CHAT
    L2B --> CHAT

    CHAT --> SCHED
    CHAT --> SEQ
    CHAT --> ANTHROPIC

    SCHED --> MCP
    SCHED --> ETHERSCAN
    SCHED --> RPC
    SCHED --> BAML
```

## Crate Dependency Graph

The workspace is organized into binary crates (executables) and library crates (shared functionality):

```mermaid
graph TD
    subgraph "Binaries"
        BIN_BACKEND[bin/backend<br/>HTTP API Server]
        BIN_CLI[bin/cli<br/>Command Line]
        BIN_TUI[bin/tui<br/>Terminal UI]
    end

    subgraph "Core Libraries"
        CHAT[aomi-chat<br/>LLM Orchestration]
        BACKEND[aomi-backend<br/>Session Management]
        TOOLS[aomi-tools<br/>Tool Scheduler]
    end

    subgraph "Integrations"
        MCP[aomi-mcp<br/>MCP Protocol]
        RAG[aomi-rag<br/>Document Search]
        BAML[aomi-baml<br/>Structured AI]
    end

    subgraph "Execution"
        ANVIL[aomi-anvil<br/>Fork Management]
        FORGE[aomi-forge<br/>Forge Agent]
        SCRIPTS[aomi-scripts<br/>Script Generation]
    end

    subgraph "Testing"
        EVAL[aomi-eval<br/>Evaluation Harness]
    end

    BIN_BACKEND --> CHAT
    BIN_BACKEND --> BACKEND
    BIN_CLI --> CHAT
    BIN_CLI --> BACKEND
    BIN_TUI --> CHAT

    CHAT --> TOOLS
    CHAT --> MCP
    CHAT --> RAG

    BACKEND --> CHAT

    FORGE --> CHAT
    FORGE --> SCRIPTS

    SCRIPTS --> ANVIL
    SCRIPTS --> BAML

    EVAL --> CHAT
    EVAL --> ANVIL
```

## Core Crates

### aomi-chat

The heart of LLM orchestration. Manages agent creation, streaming completions, and system events.

| Component | Purpose |
|-----------|---------|
| `ChatApp` | Main application wrapper |
| `ChatAppBuilder` | Builder pattern for app configuration |
| `stream_completion` | Async generator for LLM responses |
| `SystemEventQueue` | Thread-safe event buffer |
| `ChatCommand` | Streaming response variants |

### aomi-backend

Session and history management for multi-user scenarios.

| Component | Purpose |
|-----------|---------|
| `SessionManager` | Manages concurrent sessions |
| `SessionState` | Per-session state machine |
| `HistoryBackend` | Pluggable persistence |
| `ChatMessage` | Message representation |

### aomi-tools

Centralized tool management via IO Scheduler pattern.

| Component | Purpose |
|-----------|---------|
| `ToolScheduler` | Global tool registry and executor |
| `ToolApiHandler` | Per-request handler |
| `ToolResultStream` | Streaming tool results |
| `AomiApiTool` | Tool trait for single-result tools |
| `MultiStepApiTool` | Tool trait for streaming tools |

### aomi-anvil

Anvil fork management for transaction simulation.

| Component | Purpose |
|-----------|---------|
| `ForkProvider` | Managed or external fork |
| `AnvilInstance` | Spawned Anvil process |
| `ForkSnapshot` | Fork state capture |

### aomi-scripts

Forge script generation and execution.

| Component | Purpose |
|-----------|---------|
| `ForgeExecutor` | Dependency-aware executor |
| `ScriptAssembler` | Script generation |
| `SourceFetcher` | Background contract fetching |
| `ExecutionPlan` | Operation grouping |

### aomi-eval

Evaluation framework for testing agent behavior.

| Component | Purpose |
|-----------|---------|
| `EvalHarness` | Test runner |
| `EvalCase` | Test case definition |
| `Assertion` | Balance/state assertions |

## Data Flow

### Message Processing

```mermaid
sequenceDiagram
    participant User
    participant Session
    participant ChatApp
    participant LLM
    participant Scheduler
    participant Tool

    User->>Session: Send message
    Session->>ChatApp: process_message()
    ChatApp->>LLM: stream_completion()

    loop Streaming Response
        LLM-->>ChatApp: StreamingText
        ChatApp-->>Session: ChatCommand
        Session-->>User: SSE Event
    end

    LLM->>ChatApp: ToolCall request
    ChatApp->>Scheduler: handler.request()
    Scheduler->>Tool: call_with_json()
    Tool-->>Scheduler: Result
    Scheduler-->>ChatApp: ToolResultStream
    ChatApp->>LLM: Tool result

    LLM-->>ChatApp: Complete
    ChatApp-->>Session: ChatCommand::Complete
```

### System Events Flow

```mermaid
flowchart LR
    subgraph Sources
        TOOL[Tool Execution]
        WALLET[Wallet Events]
        BACKEND[Backend Status]
    end

    subgraph Queue
        SEQ[SystemEventQueue<br/>Thread-safe buffer]
    end

    subgraph Consumers
        SESSION[Session State]
        SSE[SSE Endpoint]
        AGENT[Agent History]
    end

    TOOL -->|push| SEQ
    WALLET -->|push| SEQ
    BACKEND -->|push| SEQ

    SEQ -->|drain| SESSION
    SESSION -->|inline events| SSE
    SESSION -->|inject| AGENT
```

## Key Patterns

### Builder Pattern

All agentic applications use the builder pattern for flexible configuration:

```rust
let app = ChatAppBuilder::new(&preamble).await?
    .add_tool(GetContractABI)?
    .add_tool(SimulateTransaction)?
    .add_docs_tool(sender, None).await?
    .build(skip_mcp, system_events, sender).await?;
```

### Trait-Based Abstractions

Core components use traits for testability and extensibility:

```mermaid
classDiagram
    class AomiBackend {
        <<trait>>
        +system_events() SystemEventQueue
        +process_message()
    }

    class AomiApiTool {
        <<trait>>
        +call(request) Response
        +name() str
        +description() str
    }

    class MultiStepApiTool {
        <<trait>>
        +call_stream(request, sender)
        +validate(request)
    }

    class HistoryBackend {
        <<trait>>
        +load_history(public_key)
        +save_history(messages)
    }

    ChatApp ..|> AomiBackend
    ForgeApp ..|> AomiBackend
    L2BeatApp ..|> AomiBackend

    GetContractABI ..|> AomiApiTool
    BraveSearch ..|> AomiApiTool

    ForgeExecutor ..|> MultiStepApiTool
```

### Streaming-First Design

All LLM responses and tool results stream via async generators:

```mermaid
flowchart TD
    subgraph "ChatCommand Variants"
        ST[StreamingText<br/>Incremental text]
        TC[ToolCall<br/>Tool invocation + stream]
        ATR[AsyncToolResult<br/>Multi-step result]
        COMP[Complete<br/>Response finished]
        ERR[Error<br/>Failure]
    end

    subgraph "ToolResultStream"
        SINGLE[Single Result<br/>Shared future]
        MULTI[Multi-Step<br/>mpsc channel]
    end

    TC --> SINGLE
    TC --> MULTI
    MULTI --> ATR
```

### Event Bus Architecture

The SystemEventQueue acts as a central event bus:

```mermaid
flowchart TB
    subgraph "Event Types"
        INLINE[InlineDisplay<br/>UI notifications]
        NOTICE[SystemNotice<br/>Status messages]
        ERROR[SystemError<br/>Error messages]
        ASYNC[AsyncUpdate<br/>Background updates]
    end

    subgraph "Routing"
        QUEUE[SystemEventQueue]
        HANDLER[handle_system_event]
    end

    subgraph "Destinations"
        ACTIVE[active_system_events<br/>Immediate UI]
        PENDING[pending_async_events<br/>SSE broadcast]
    end

    INLINE --> QUEUE
    NOTICE --> QUEUE
    ERROR --> QUEUE
    ASYNC --> QUEUE

    QUEUE --> HANDLER

    HANDLER -->|inline| ACTIVE
    HANDLER -->|async| PENDING
```

## External Integrations

| Service | Crate | Purpose |
|---------|-------|---------|
| Anthropic Claude | aomi-chat | LLM inference |
| MCP Server | aomi-mcp | Extended tool protocol |
| Etherscan | aomi-tools | Contract data |
| Brave Search | aomi-tools | Web search |
| BAML Server | aomi-baml | Structured AI outputs |
| Alchemy/Infura | aomi-tools | RPC endpoints |

## Configuration

### Environment Variables

| Variable | Required | Purpose |
|----------|----------|---------|
| `ANTHROPIC_API_KEY` | Yes | Claude API access |
| `DATABASE_URL` | Yes | PostgreSQL/SQLite connection |
| `BRAVE_SEARCH_API_KEY` | No | Web search |
| `ETHERSCAN_API_KEY` | No | Contract data |
| `ALCHEMY_API_KEY` | No | RPC endpoints |
| `BAML_SERVER_URL` | No | Structured AI |

### Network Configuration

Networks are configured via `config.yaml`:

```yaml
networks:
  ethereum:
    rpc_url: "https://eth-mainnet.g.alchemy.com/v2/{key}"
    chain_id: 1
  base:
    rpc_url: "https://base-mainnet.g.alchemy.com/v2/{key}"
    chain_id: 8453
```

## Build & Run

```bash
# Development
cargo run --bin backend     # HTTP API server
cargo run --bin cli         # Interactive CLI
cargo run --bin tui         # Terminal UI

# Testing
cargo test --all            # All tests
cargo test --package aomi-tools  # Specific crate

# Production
docker build --target backend-runtime -t aomi-backend .
```
